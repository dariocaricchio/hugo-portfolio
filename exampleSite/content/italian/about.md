---
title : "SU DI ME"
image : "https://res.cloudinary.com/dario-caricchio/image/upload/v1621548143/backgrounds/portrait_dlnmps.jpg" # "images/backgrounds/portrait.jpg"
# button
button:
  enable : true
  label : "SCARICA IL MIO CV"
  link : "https://drive.google.com/file/d/1Wq-RNOlxgV-q_rxMYXxeN9n7BlxRGQE7/view?usp=sharing"

########################### Experience ##############################
experience:
  enable : true
  title : "ESPERIENZE PRINCIPALI"
  experience_list:

    # experience item loop
    - name : "Big Data Engineer - Consulente"
      company : "Vodafone Italia per conto di NTT Data Italia ~ Capgemini"
      duration : "25/10/2021 â€“ 31/05/2022 ~ 06/06/2022 â€“ in corso"
      content : "â–º Analisi e sviluppo di job Spark in Scala i quali vengono eseguiti su cluster Dataproc su GCP al fine di processare dati presenti in Google Storage per mascherare informazioni sensibili.<br>
      â–º Orchestrazione, sviluppo, mantenimento e verifica DAG di Airflow in Python con PySpark attraverso i servizi GCP quali Composer, Dataproc e Google Storage per l'analisi dei dati.<br>
      â–º Operazioni di tuning, bug-fixing, analisi delle performance e miglioramento di job Spark preesistenti.<br>
      â–º Progettazione, analisi e sviluppo di funzioni, stored procedures e tabelle in BigQuery usando SQL e vari connettori, come Airflow, Google Storage e Pyspark."

    # experience item loop
    - name : "ML Engineer - Consulente"
      company : "Azienda Americana-Britannica-Svizzera Multinazionale di Distribuzione Farmaceutica per conto di Azienda Italiana di Digital Solution"
      duration : "30/03/2021 â€“ 22/10/2021"
      content : "â–º Analisi e sviluppo di un processo automatico per l'individuazione di inconsistenze dello schema e per
        rilevare l'esistenza di chiavi primarie duplicate utilizzando il linguaggio Python e PySpark e in generale le
        tecnologie Azure attraverso sia Databricks Workspace sia sviluppando in ambiente locale attraverso
        databricks-connect e databricks-cli.<br>
      â–º Generazione di delta table curate a partire da un ADLS storage account curato; le tabelle finali sono
        equivalenti alle corrispettive tabelle curate presenti su Synapse (ADW).<br>
      â–º Valutazione di algoritmi di Intelligenza Artificiale giÃ  in essere."


    # experience item loop
    - name : "Big Data Engineer - Consulente"
      company : "Azienda Britannica Multinazionale di Media e Telecomunicazioni (divisione tedesca e austriaca) per conto di Azienda Giapponese Multinazionale di Servizi IT e Consulenza (divisione italiana)"
      duration : "1/11/2020 - 15/03/2021"
      content : "â–º Ingegnere Big Data su progetto cliente utilizzando Scala e Java come linguaggi di programmazione e Google Cloud Platform.<br>
      â–º Design e sviluppo di processi Spark in Scala usando i servizi GCP come Google Cloud Storage, Pub/Sub, Google DLP ed altri.<br>
      â–º Design e sviluppo di processi Apache Beam per Dataflow usando SCIO, una libreria di Beam in Scala, Kafka e tecnologie GCS per l'elaborazione dei dati all'interno dell'ingestion layer in contesti sia batch che streaming.<br>
      â–º Design e sviluppo di un POC per evidenziare varie strategie legate alla sicurezza dei precessi Dataflow attraverso Google KMS, DLP e la libreria crittografica Google Tink."

    # experience item loop
    - name : "Big Data Engineer - Consulente"
      company : "Azienda Italiana Multinazionale di ElettricitÃ -Gas per conto di Azieda Francese Multinazionale di Servizi IT e Consulenza"
      duration : "23/09/2019 â€“ 15/10/2020"
      content : 'â–º Ingegnere Big Data su progetto Enel Next utilizzando Scala e Java come linguaggi di programmazione e ambiente Hadoop Cloudera.<br>
      â–º Design e sviluppo di processi Spark attraverso una piattaforma Scala proprietaria costruita al di sopra dello Spark core.<br>
      â–º Design e sviluppo di strumenti per la Data Quality usando le standard Spark Core API (spark 2.4.5 e Scala 2.11.12).<br>
      â–º Design e sviluppo di strumenti per il Reporting basandosi su strumenti quali HIVE, Impala, file Parquet/ORC/Avro su S3 e HDFS al fine di ottenere la materializzazione dei dataset, Data Visualization e CSV/Excel file export.'

    # experience item loop
    - name : "Intern"
      company : "Hamlyn Centre - Imperial College of London"
      duration : "15/02/2019 â€“ 18/07/2019"
      content : 'â–º Borsa di studio per il programma "Erasmus+ Traineeship BET for jobs".<br>
      â–º Sviluppo di un algoritmo di compressione discusso in letteratura per sensori ECG con linguaggio di programmazione C.<br>
      â–º Lavoro su "abnormal gait detection" (rilevamento della camminata anomala) usando il linguaggio di programmazione Python e relative librerie, combinando algoritmi di machine learning con metodologie per il pre-processing, feature extraction, dataset creation, data visualization, discrete wavelet transformation e classificazione.'

############################### Skill #################################
skill:
  enable : true
  title : "COMPETENZE"
  skill_list:
    # skill item loop
    - name : "Computer Engineering"
      percentage : "98%"

    # skill item loop
    - name : "Big Data"
      percentage : "85%"

    # skill item loop
    - name : "Software Development"
      percentage : "90%"

    # skill item loop
    - name : "Machine Learning"
      percentage : "70%"

    # skill item loop
    - name : "Servizi Cloud"
      percentage : "80%"

  subtitle: CERTIFICAZIONI
  description: "Puoi dare un'occhiata alle mie certificazioni sul mio [Profilo Linkedin](https://www.linkedin.com/in/dariocaricchio/details/certifications/)."


# custom style
custom_class: ""
custom_attributes: ""
custom_css: ""
---

Sono Dario, un Ingegnere Informatico. Adoro risolvere problemi attraverso la programmazione e lo sviluppo di codice, Ãˆ la mia passione. Sono sempre disponibile per progettare e sviluppare soluzioni software di varia natura. Posso anche aiutare a descrivere te stesso nel miglior modo possibile con un sito web.<br>Non importa quanto possa essere difficile un problema, farÃ² tutto il possibile per trovare una soluzione e completare il lavoro. Rimaniamo in contatto! ðŸ˜Š
